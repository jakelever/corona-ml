---
title: "Analyzing the vast coronavirus literature with CoronaCentral"
#output: 
#  bookdown::pdf_document2
#  html_document: 
#    fig_caption: yes
#params:
#   region: east
#editor_options: 
#  chunk_output_type: console
site: bookdown::bookdown_site
documentclass: book
#output:
#  bookdown::html_document2
#  bookdown::word_document2
#  bookdown::pdf_document2
output:
   pdf_document:
      latex_engine: xelatex
bibliography: bibliography.bib
csl: pnas.csl
---



```{r dependencies, include=FALSE}
source('dependencies.R')

theme_set(theme_pubr())

knitr::opts_chunk$set(echo = TRUE)
```

```{r date_info, echo=F}
update_date <- file.info('coronacentral.withaltmetric.json')$mtime
update_date_nice <- format(update_date, format="%d %B %Y")
```


```{python pythonTime, echo=F}
import json
import pandas as pd
from collections import Counter,defaultdict

preprint_journals = ['arXiv','bioRxiv','ChemRxiv','medRxiv']

with open('coronacentral.withaltmetric.json') as f:
  documents = json.load(f)


document_count = len(documents)
longhaul_count = len( [ d for d in documents if 'Long Haul' in d['topics'] ])

retractions_count = len( [ d for d in documents if 'Retracted' in d['topics'] ])

comment_count = len( [ d for d in documents if 'Comment/Editorial' in d['articletypes'] ])
comment_perc = round(100*comment_count/len(documents),1)

preprint_count = len( [ d for d in documents if d['journal'] in preprint_journals ])
preprint_perc = round(100*preprint_count/len(documents),1)

sarscov2_preprint_count = len( [ d for d in documents if d['journal'] in preprint_journals and 'SARS-CoV-2' in d['viruses'] ])
sarscov2_count = len( [ d for d in documents if 'SARS-CoV-2' in d['viruses'] ])
sarscov2_preprint_perc = round(100*sarscov2_preprint_count/sarscov2_count,1)

all_articletypes = sorted(set(c for d in documents for c in d['articletypes']))
all_topics = sorted(set(c for d in documents for c in d['topics']))

articletype_counts = Counter( c for d in documents for c in d['articletypes'] )
topic_counts = Counter( c for d in documents for c in d['topics'] )

comment_topic_counts = Counter( c for d in documents if 'Comment/Editorial' in d['articletypes'] for c in d['topics'] )

topic_df = pd.DataFrame(data={'topic':list(topic_counts.keys()),'count':list(topic_counts.values())})

articletype_df = pd.DataFrame(data={'articletype':list(articletype_counts.keys()),'count':list(articletype_counts.values())})

comment_topic_df = pd.DataFrame(data={'topic':list(comment_topic_counts.keys()),'count':list(comment_topic_counts.values())})

topic_and_articletype_counts = defaultdict(Counter)
for d in documents:
  for articletype in d['articletypes']:
    for topic in d['topics']:
      topic_and_articletype_counts[topic][articletype] += 1
      
topic_and_articletype_df = pd.DataFrame( [ [articletype,topic,topic_and_articletype_counts[topic][articletype],sum(topic_and_articletype_counts[topic].values())] for topic in topic_and_articletype_counts for articletype in topic_and_articletype_counts[topic] ], columns=['articletype','topic','count','total_for_topic'] )

papers_by_month = Counter()
topics_by_month = defaultdict(Counter)
for d in documents:
  viruses = set([ e['normalized'] for e in d['entities'] if e['type'] == 'Virus' ])
  d['viruses'] = viruses
  if not 'SARS-CoV-2' in viruses:
    continue
    
  if d['publish_year'] != 2020 or not d['publish_month']:
    continue
    
  month = d['publish_month']
  papers_by_month[month] += 1
  
  for c in d['topics']:
    topics_by_month[month][c] += 1
    
top_topics = [ c for c,count in topic_counts.most_common(10) ]
    
topics_by_month_data = []
for month in sorted(topics_by_month.keys()):
  total_in_month = papers_by_month[month]
  for c in sorted(top_topics):
    count = topics_by_month[month][c]
    perc_in_month = round(100*count/total_in_month,2)
    topics_by_month_data.append( [month,c,count,perc_in_month,total_in_month] )
    
topics_by_month_df = pd.DataFrame(topics_by_month_data, columns=['month','category','count','perc_in_month','total_in_month'])

publications_by_date = Counter()
for d in documents:
  if not d['publish_year'] or not d['publish_month']:
    continue
    
  date = "%04d/%02d/01" % (int(d['publish_year']),int(d['publish_month']))
  
  for v in d['viruses']:
    publications_by_date[(v,date)] += 1
    
publications_by_date_df = pd.DataFrame( [ [v,date,count] for (v,date),count in publications_by_date.items()], columns=['virus','date','count'])

sarscov2_may_count = publications_by_date[('SARS-CoV-2','2020/05/01')]

papers_by_month = Counter()
comment_topics_counts = Counter()
research_topics_counts = Counter()
comment_topics_by_month = defaultdict(Counter)
research_topics_by_month = defaultdict(Counter)
for d in documents:
  if not 'SARS-CoV-2' in d['viruses']:
    continue
    
  if not d['publish_month']:
    continue
  if d['publish_year'] < 2020:
    continue
  #elif d['publish_year'] == 2021 and d['publish_month'] <:
  #  pass
  #else:
  #  continue
    
  date = "%04d/%02d/01" % (int(d['publish_year']),int(d['publish_month']))
  #month = d['publish_month']
  papers_by_month[date] += 1
  
  if 'Comment/Editorial' in d['articletypes']:
    for topic in d['topics']:
      comment_topics_counts[topic] += 1
      comment_topics_by_month[date][topic] += 1
  elif 'Research' in d['articletypes']:
    for topic in d['topics']:
      research_topics_counts[topic] += 1
      research_topics_by_month[date][topic] += 1
      
topic_trends_data = []
for date in comment_topics_by_month:
  top_topics = [ c for c,_ in comment_topics_counts.most_common(5) ]
  for c in top_topics:
    topic_trends_data.append( ['Comment/Editorial',date,c,comment_topics_by_month[date][c],papers_by_month[date]] )
for date in research_topics_by_month:
  top_topics = [ c for c,_ in research_topics_counts.most_common(5) ]
  for c in top_topics:
    topic_trends_data.append( ['Original Research',date,c,research_topics_by_month[date][c],papers_by_month[date]] )
    
topic_trends_df = pd.DataFrame(topic_trends_data,columns=['articletype','date','topic','count','total_per_month'])

longhaul_df = pd.DataFrame( [ [ date, research_topics_by_month[date]['Long Haul'] ] for date in research_topics_by_month ], columns=['date','count'])

entity_counts = defaultdict(Counter)
entity_counts_by_virus = defaultdict(lambda : defaultdict(Counter))
for d in documents:
  viruses = set([ e['normalized'] for e in d['entities'] if e['type'] == 'Virus' ])
  other_entities = set([ (e['type'],e['normalized']) for e in d['entities'] if e['type'] != 'Virus' ])
  for v in viruses:
    for e_type,e_normalized in other_entities:
      entity_counts_by_virus[v][e_type][e_normalized] += 1
  
  for e_type,e_normalized in other_entities:
    entity_counts[e_type][e_normalized] += 1
    
entity_counts_data = []
for v in entity_counts_by_virus:
  for e_type in entity_counts:
    entity_counts_data += [ [v,e_type,e,entity_counts_by_virus[v][e_type][e],total_across_viruses] for e,total_across_viruses in entity_counts[e_type].most_common(15) ]
  
entity_counts_df = pd.DataFrame(entity_counts_data,columns=['virus','type','name','count','count_for_all_viruses'])

preprint_counts = defaultdict(Counter)
journal_counts = Counter()
for d in documents:
  j = d['journal'] if d['journal'] in preprint_journals else 'Non-preprint'
  if d['journal']:
    journal_counts[d['journal']] += 1
    
  for topic in d['topics']:
    preprint_counts[j][topic] += 1
    
preprints_data = []
for j in preprint_counts:
  total = sum(preprint_counts[j].values())
  assigned = 0
  for c in preprint_counts[j]:
    perc = 100 * preprint_counts[j][c] / total
    #if perc > 5:
    preprints_data.append( [j,c,preprint_counts[j][c],perc] )
    assigned += preprint_counts[j][c]
      
  count = total - assigned
  perc = 100 * count / total
  #preprints_data.append( [j,'Other',count,perc] )

preprints_df = pd.DataFrame( preprints_data, columns=['journal','category','count','perc'] )

journals_df = pd.DataFrame( journal_counts.most_common(20), columns=['journal','count'] )

docs_with_score = [ (d['altmetric']['score'],d) for d in documents if 'SARS-CoV-2' in d['viruses'] and 'altmetric' in d and 'score' in d['altmetric'] ]
docs_with_score = sorted(docs_with_score,reverse=True,key=lambda x:x[0])

topic_counts_at_rank = Counter( c for _,d in docs_with_score[:100] for c in d['topics'] )

altmetric_df = pd.DataFrame(topic_counts_at_rank.most_common(len(topic_counts_at_rank)),columns=['topic','count'])

```

```{r figurecode, echo=F, eval=T}

##################
# SURGING TOPICS #
##################

pubcounts <- read.table('yearPubCounts.tsv',sep='\t',header=F,quote='')
colnames(pubcounts) <- c('year','count')
rownames(pubcounts) <- pubcounts$year

surgingtopics <- read.table('yearEntityDiffs_top.tsv',sep='\t',header=F,quote='')
colnames(surgingtopics) <- c('diff','before','after','before_year','entity_type','entity_id','name')
surgingtopics$after_year <- surgingtopics$before_year + 1

surgingtopics$label <- paste(surgingtopics$name,' (', surgingtopics$before_year,'-', surgingtopics$before_year+1, ')',sep='')
surgingtopics$label <- factor(surgingtopics$label,unique(surgingtopics$label[order(surgingtopics$diff,decreasing=T)]))


surgingtopics$before_normalized <- 100*surgingtopics$before / pubcounts[as.character(surgingtopics$before_year),'count']
surgingtopics$after_normalized <- 100*surgingtopics$after / pubcounts[as.character(surgingtopics$after_year),'count']

surgingtopics$diff_normalized = surgingtopics$after_normalized - surgingtopics$before_normalized

top_all <- surgingtopics %>% 
  arrange(desc(diff_normalized)) %>%
  slice_head(n=8)

top_all_long <- melt(top_all[,c('label','before_normalized','after_normalized')], id.vars="label")
top_all_long$label <- factor(top_all_long$label, levels=top_all$label)

plots_surgingtopics <- ggplot(top_all_long, aes(x=label, y=value)) + 
  geom_line(arrow = arrow(), size = 1.5) +
  geom_point(size=4, aes(colour=variable)) +
  xlab("Concept and years of change") + ylab("% of PubMed mentioning concept") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  theme(legend.position = "none") + 
  theme(panel.grid.major.x = element_blank(),
        panel.background = element_blank()) #, axis.line = element_line(colour = "black")


#################################
# Topic / Article Type Overview #
#################################

topic_count <- nrow(py$topic_df)
articletype_count <- nrow(py$articletype_df)

py$topic_df$topic <- factor(py$topic_df$topic, levels=py$topic_df$topic[order(py$topic_df$count,decreasing=F)])

plots_topicoverview <- ggplot(data=py$topic_df, aes(x=topic, y=count)) +
  geom_bar(stat="identity", fill="steelblue") +
  xlab("Topic") + ylab("# of papers") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        panel.background = element_blank()) +
  scale_fill_brewer(palette = "Set2", name='') +
  theme(
    axis.text.y = element_text(hjust = 1, margin=margin(0,-10,0,0)),
    axis.ticks.y = element_blank()) +
  coord_flip()

py$articletype_df$articletype <- factor(py$articletype_df$articletype, levels=py$articletype_df$articletype[order(py$articletype_df$count,decreasing=F)])

plots_articletypeoverview <- ggplot(data=py$articletype_df, aes(x=articletype, y=count)) +
  geom_bar(stat="identity", fill="steelblue") +
  xlab("Article Type") + ylab("# of papers") +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        panel.background = element_blank()) +
  scale_fill_brewer(palette = "Set2", name='') +
  theme(
    axis.text.y = element_text(hjust = 1, margin=margin(0,-10,0,0)),
    axis.ticks.y = element_blank()) +
  coord_flip()

##########################
# Topics by Article Type #
##########################

py$topic_and_articletype_df$perc <- 100 * py$topic_and_articletype_df$count / py$topic_and_articletype_df$total_for_topic

comment_only <- py$topic_and_articletype_df[py$topic_and_articletype_df$articletype=='Comment/Editorial',]

py$topic_and_articletype_df$topic <- factor(py$topic_and_articletype_df$topic, levels=comment_only$topic[order(comment_only$perc,decreasing=T)])

plots_topicbyarticletype <- ggplot(data=py$topic_and_articletype_df, aes(x=topic, y=perc, fill=articletype)) +
  geom_bar(stat="identity") +
  xlab("Category") + ylab("# of papers") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_blank()) +
  theme(axis.text.y = element_text(hjust = 1, margin=margin(0,0,0,0)),
        axis.ticks.y = element_blank()) +
  scale_fill_brewer(palette = "RdYlBu", name="Article Type")+
  theme(plot.margin=unit(c(.1,.1,.1,1.),"cm")) +
  coord_flip()

##################
# Category Trend #
##################

palette <- brewer.pal(10, "Spectral")[1:8]

topic_trends_df <- as.data.frame(py$topic_trends_df)
topic_trends_df$date <- as.Date(topic_trends_df$date)
topic_trends_df$perc_in_month <- 100 * topic_trends_df$count / topic_trends_df$total_per_month

topic_trends_df$articletype <- factor(topic_trends_df$articletype,levels=c('Original Research','Comment/Editorial'))

plots_topictrend <- ggplot(data=topic_trends_df, aes(x=date, y=perc_in_month, group=topic)) +
  geom_line(aes(color=topic),size=2) +
  facet_wrap(~ articletype, ncol=1, scales = "free") + 
  xlab("Month") + ylab("% of SARS-CoV-2 papers") +
  theme_minimal() + 
  #theme(panel.grid.minor = element_blank()) +
  scale_colour_brewer(palette = "Set2", name="Topic") +
  #scale_y_continuous(limits = c(0,NA)) + 
  scale_x_date(date_labels = "%b '%y",
               minor_breaks = as.Date(sort(unique(topic_trends_df$date)))) #+ 
  #theme(panel.spacing = unit(2, "lines"))
  #scale_fill_manual(values=palette)

###################
# Entity Overview #
###################

entity_names <- py$entity_counts_df[!duplicated(py$entity_counts_df$name),c('name','count_for_all_viruses')]

entity_name_counts <- aggregate(py$entity_counts_df$count, by=list(name=py$entity_counts_df$name), FUN=sum)

py$entity_counts_df$name <- factor(py$entity_counts_df$name, levels=entity_name_counts$name[order(entity_name_counts$x,decreasing=T)])

py$entity_counts_df$virus <- factor(py$entity_counts_df$virus,levels=c('SARS-CoV-2','MERS-CoV','SARS-CoV'))

plots_entityoverview <- ggplot(data=py$entity_counts_df, aes(x=name, y=count, fill=virus)) +
  geom_bar(stat="identity", aes(fill=virus)) +
  facet_wrap(~ type, scales = "free") + 
  xlab("") + ylab("# of papers") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  scale_fill_manual(values=c("#66c2a5", "#8da0cb", "#fc8d62"), name="Virus")+
  
  theme(plot.margin=unit(c(.5,.5,.5,.5),"cm")) +
  theme(legend.position = "top",
    legend.direction = "horizontal")


############
# Journals #
############


py$journals_df$is_preprint <- factor(py$journals_df$journal %in% c('arXiv','bioRxiv','ChemRxiv','medRxiv'),
                                     labels=c("Not Preprint","Preprint"))

py$journals_df$journal_short <- as.character(py$journals_df$journal)
py$journals_df$journal_short[stri_length(py$journals_df$journal_short)>=50] <- paste0(substr(py$journals_df$journal_short[stri_length(py$journals_df$journal_short)>=50],0,47),'...')

py$journals_df$journal_short <- factor(py$journals_df$journal_short,levels=py$journals_df$journal_short[order(py$journals_df$count,decreasing=T)])

plots_journals <- ggplot(data=py$journals_df, aes(x=journal_short, y=count, fill=is_preprint)) +
  geom_bar(stat="identity", aes(fill=is_preprint)) +
  xlab("") + ylab("# of papers") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust=1),
        panel.grid.major.x = element_blank()) +
  scale_fill_brewer(palette = "Set1",name='') + 
  theme(legend.position = c(.95, .95),
    legend.justification = c("right", "top"),
    legend.box.just = "right",
    legend.margin = margin(6, 6, 6, 6),
    legend.background = element_rect(fill="white", size=0.5, linetype=0))

#############
# Preprints #
#############

palette <- c(brewer.pal(9, "Set1"),brewer.pal(6, "Set3"))

#palette[10] <- '#333333'
palette[10] <- '#eeeeee'

#ggplot(data=py$preprints_df, aes(x=journal, y=perc, fill=category)) +
#  geom_bar(position="stack", stat="identity", aes(fill=category)) +
#  xlab("Source") + ylab("% of predicted topics") +
#  theme_minimal()  +
  #scale_fill_brewer(palette = "RdYlBu") + 
#  scale_fill_manual(values=palette, name="Topic")


#is_alluvia_form(py$preprint_category_alluvial_pd, axes = 1:2, silent = TRUE)

preprint_data <- py$preprints_df %>% filter(journal != 'Non-preprint')

preprint_data <- preprint_data %>%
    group_by(category)                   %>% # Group by category
    mutate(category_count = sum(count)) %>% # Sum by category
    ungroup

preprint_data[preprint_data$category_count < 100, 'category'] <- 'Other'

#preprint_data$journal <- as.factor(preprint_data$journal)
#$category <- as.factor(preprint_data$category)

preprint_data <- preprint_data %>%
    group_by(journal,category)                   %>% # Group by category
    mutate(count = sum(count)) %>% # Sum by category
    filter(row_number() == 1) %>%
    ungroup

palette <- c(brewer.pal(6, "Set1"),brewer.pal(6, "Set2"),brewer.pal(6, "Set3"))
palette <- grep("#FFFFB3",palette,invert=T,value=T)

plots_preprints <- ggplot(preprint_data,
       aes(y = count, axis2 = category, axis1 = journal)) +
  geom_alluvium(aes(fill = category), width = 1/2) +
  geom_stratum(width = 1/2) +
  geom_label(stat = "stratum", aes(label = after_stat(stratum))) +
  #geom_text(stat = "stratum", aes(label = after_stat(stratum)),
  #          reverse = FALSE) +
  theme_minimal() +
  theme(legend.position = "none") + 
  scale_fill_manual(values=palette) + 
  #scale_fill_brewer(palette='Set2') + 
  xlab('') + ylab('') + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks=element_blank()) +
  scale_x_continuous(expand=c(0,0)) + 
        scale_y_continuous(expand=c(0,0))


################
# Pubs by Time #
################

pubsbytime_data <- as.data.frame(py$publications_by_date_df)
pubsbytime_data$dateObj <- as.Date(as.character(pubsbytime_data$date))
pubsbytime_data$virus <- factor(pubsbytime_data$virus,levels=c('SARS-CoV-2','MERS-CoV','SARS-CoV'))

plots_pubsbytime <- ggplot(data=pubsbytime_data, aes(x=dateObj, y=count)) +
  geom_bar(stat="identity", aes(fill=virus)) +
  facet_grid(virus ~ ., scales = "free", switch='y') + 
  xlab("Date") + ylab("# of papers per month") +
  theme_minimal() + 
  scale_fill_manual(values=c("#66c2a5", "#8da0cb", "#fc8d62")) +
  theme(legend.position = "none") + 
  theme(panel.spacing = unit(1, "lines")) + 
  scale_y_continuous(position = "right") + 
  scale_x_date(minor_breaks = as.Date(paste0(2000:2021,'-01-01')))


#############
# Altmetric #
#############

altmetric_data <- py$altmetric_df

altmetric_data$topic <- factor(altmetric_data$topic,
                                  levels=altmetric_data$topic[order(altmetric_data$count)])

plots_altmetric <- ggplot(altmetric_data, aes(x=topic, y=count)) +
  geom_bar(stat="identity", fill="steelblue") +
  xlab("") + ylab("# of papers in Top 100 by Altmetric score") +
  theme_minimal() +
  scale_fill_manual(values=palette,name="Topic") +
  coord_flip() +
  theme(axis.text.y = element_text(hjust = 1, margin=margin(0,-10,0,0)),
        panel.grid.major.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_y_continuous(breaks = seq(0, 100, 5), minor_breaks=seq(0, 100, 1))
        

```

## Abstract {-}

The global SARS-CoV-2 pandemic has caused a surge in research exploring all aspects of the virus and its effects on human health. The overwhelming rate of publication means that human researchers are unable to keep abreast of the research. To ameliorate this, we present the CoronaCentral resource which uses machine learning to process the research literature on SARS-CoV-2 along with articles on SARS-CoV and MERS-CoV. We break the literature down into useful topics and article types and enable analysis of the contents, pace, and emphasis of research during the crisis. These topics cover therapeutics, disease forecasting as well as growing areas such as "Long COVID" and studies of inequality. Using this data, we compare topics that appear in original research articles compared to commentaries and other article types. Finally, using Altmetric data, we identify the topics that have gained the most media attention. This resource, available at https://coronacentral.ai, is updated daily.

## Background {-}

The pandemic has led to the greatest surge in biomedical research on a single topic in documented history (Fig \@ref(fig:fig1)A). This research is valuable both to current and future researchers as they examine the long term effects of the virus on different aspects of society. Unfortunately, the vast scale of the literature makes it challenging to navigate. Machine learning systems, that can automatically identify topics and article types of papers, would greatly benefit researchers who are searching for relevant coronavirus research.

Analysis of the coronavirus literature was spurred by the availability of the CORD-19 literature dataset [@wang2020cord] and access to PubMed. Many approaches have used topic modelling techniques to extracted unsupervised topics of discussions [@doanvo2020;@bras2020visualising]. The TREC-COVID shared task provided a number of information retrieval challenges on specific COVID-19 topics [@roberts2020trec]. Other research implements advanced search functionality to provide keyword search [@zhang2020covidex;@verspoor2020covid]. LitCovid provides a limited set of categories to index all literature [@chen2020keep].

Our approach improves on the existing methods, including LitCovid, by covering a larger set of papers with the inclusion of PubMed and CORD-19 along with SARS/MERS papers, a larger and more specific set of topics, identification of article types (e.g. Commentaries and Editorials), integration of Altmetric esteem data and indexing by a wide set of biomedical terms (e.g. Drugs, Symptoms, Viral Lineages, etc). All data is available for download and the full codebase is available on GitHub.

## Results {-}

To provide more detailed and higher quality topics, we pursue a supervised learning approach and have annotated over 3,200 articles with a set of `r topic_count` topics and `r articletype_count` article types (Fig \@ref(fig:fig1)B/C). Individual papers may be tagged with multiple topics and are typically tagged with a single article type. Using a BERT-based document multi-label classification method, we achieved a micro-F1 score of 0.68 with micro-precision of 0.76 and micro-recall of 0.62. A breakdown of the performance by topics and article types shows varying quality of performance with some performing very well (e.g. contact tracing and forecasting) and others performing poorly (e.g. long haul) likely due to extremely low representation in the test set. Several other topics and article types are identified using simple rule-based methods including Clinical Trials and Retractions. 

```{r fig1, echo=F, eval=T, fig.width=11, fig.asp=1.1,  fig.cap='(ref:fig1)'}

ggdraw() +
  draw_plot(plots_surgingtopics,      x = 0.02,  y = .7, width = .48,  height = .3) +
  draw_plot(plots_topicoverview,   x = 0,  y = 0.2,  width = .5, height = .5) +
  draw_plot(plots_articletypeoverview,   x = 0,  y = 0.,  width = .5, height = .2) +
  draw_plot(plots_topictrend,      x = 0.52, y = .7,  width = .48,  height = .3) +
  draw_plot(plots_topicbyarticletype, x = .5, y = 0.,  width = .5,  height = .7) +
  draw_plot_label(label = c("A", "B", "C", "D","E"), size = 15,
                  x = c(0, 0, 0, 0.5, 0.5), y = c(1., .7, .2, 1, .7))
```
(ref:fig1) Overview of research trends and important topics (A) Largest year-on-year changes in the percentage of papers that mention a biomedical concept using data from PubTator [@wei2019pubtator]. (B) Frequency of each topic and (C) article type across the entire coronavirus literature. (D) The trajectories of the top five topics for original research and comment/editorial articles for SARS-CoV-2. (E) Different proportions of article types for each topic

As of `r update_date_nice`, CoronaCentral covers `r  prettyNum(py$document_count,big.mark=",",scientific=FALSE)` papers. The top topic (Fig \@ref(fig:fig1)B), Clinical Reports, covers articles describing patients and their symptoms, including case reports. The second top topic, the Effect on Medical Specialties, covers how specific specialties (e.g. oncology) must adapt to the pandemic. While other approaches have focused on viral biology, we made a specific effort to also identify papers that discuss the societal impacts including the psychological aspects of the pandemic, the inequality highlighted by the pandemic, and the long-term health effects of COVID. This final topic, also known as Long COVID, is covered by the Long Haul topic which currently includes `r py$longhaul_count` papers. We find the first papers discussing the possible long-term consequences of COVID appeared in April 2020 [@kiekens2020rehabilitation]. Since then, there has been a slow steady increase in publications on the challenge of "Long COVID" with ~30 papers per month recently. While all the annotated Long Haul documents used to train our system focus on SARS-CoV-2, our system finds 12 papers for the long-term consequences of SARS-CoV and one for MERS-CoV. Our approach also identifies the article type, given our estimate that `r py$comment_perc`% of coronavirus publications are comments or editorials and not original research (Fig \@ref(fig:fig1)C).

The predicted topics reveal the publication trend during the pandemic (Fig \@ref(fig:fig1)D). Early research focused on disease forecasting and modeling and has steadily decreased as a proportion compared to other topics, such as the risk factors of coronavirus, which have increased. Clinical Reports have been steady, as a proportion, throughout the pandemic. In commentaries and editorials, the main topic has been the effect on different medical specialties.

Figure \@ref(fig:fig1)E shows that different topics have drastically different distributions of article types. While almost all papers that look at forecasting or modeling the pandemic are original research, about half of the health policy articles are commentary or editorials. Notable topics with larger proportions of reviews are the more science-focused topics including Molecular Biology, Drug Targets, and Vaccines.

To identify highly discussed papers and make the resource more navigable, we integrate Altmetric data to identify papers that have received wide coverage in mass and social media. Figure \@ref(fig:fig2)A shows the breakdown of topics in the 100 papers with highest Altmetric scores. The distribution looks very different from the overall distribution of coronavirus literature, reflecting the interest in understanding treatments and prevention methods.

```{r fig2, echo=F, eval=T, fig.width=11, fig.asp=1,  fig.cap='(ref:fig2)'}

ggdraw() +
  draw_plot(plots_altmetric,       x = 0, y = .5,  width = .5,  height = .5) +
  draw_plot(plots_preprints,      x = .5,  y = 0,  width = .5, height = 1) +
  draw_plot(plots_journals,      x = 0, y = 0,  width = .5,  height = .5) +
  draw_plot_label(label = c("A", "B", "C"), size = 15,
                  x = c(0, 0, .5), y = c(1, .5, 1))
```
(ref:fig2) Communication of research has changed with a greater emphasis on social media and preprint servers (A) The number of papers categorized with each topic in the 100 papers with highest Altmetric scores (B) Top journals and preprint servers (C) Topic breakdown for each preprint server and non-preprint peer-reviewed journals. Infrequent topics in preprints are grouped in Other.

## Discussion {-}

The Covid pandemic has revealed many challenges of communicating important research during a health crisis. Pre-Covid methods for literature search often rely on long-term metrics like citation counts to prioritize search results. These approaches are unsuitable in a fast-moving environment. By integrating Altmetric scores with detailed topic and article type information, CoronaCentral enables users to narrow their focus to identify important papers in a timely manner. As the pandemic continues, monitoring of the trending articles will help identify new topics and verify that topic drift does not noticeably reduce machine learning quality.

This crisis has also highlighted the importance of preprint servers (Fig \@ref(fig:fig2)B) as they lead the list of article sources. However, they only account for `r py$sarscov2_preprint_perc`% (`r  prettyNum(py$sarscov2_preprint_count,big.mark=",",scientific=FALSE)`/`r  prettyNum(py$sarscov2_count,big.mark=",",scientific=FALSE)`) of all SARS-CoV-2 articles. We find that the indexed preprint servers were used for dramatically different topics (Fig \@ref(fig:fig2)C). As might be expected the more mathematically focused papers, such as Forecasting/Modelling have been submitted to arXiv. Molecular biology tends to go to bioRxiv, therapeutics to ChemRxiv and a diverse set of clinical topics to MedRxiv.

## Materials and Methods {-}

The documents from PubMed and CORD-19 are processed with a pipeline for topic and article type prediction, data cleaning, and other steps described in the Extended Methods. More detailed information is available at the GitHub repository (https://github.com/jakelever/corona-ml) with data at https://doi.org/10.5281/zenodo.4383289.

## Extended Methods {-}

**Data Collection:** The CORD-19 dataset [@wang2020cord] and PubMed articles containing relevant coronavirus keywords are downloaded daily. Articles are cleaned to fix Unicode issues, remove erroneous text from abstracts, and identify publication dates. Non-English language articles are filtered out using a rule-based system based on sets of stopwords in multiple languages. To remove duplicates, documents were merged using identifiers, combinations of title and journal, and other metadata. Metadata from the publishers’ websites is also integrated which enables normalization of consistent journal names and further abstract text fixes. Additional manual fixes to title, abstracts, and metadata are applied to the corpus. Altmetric data is updated regularly and integrated with the data.

**Topics and Article Types:** Manual evaluation of an initial 1000 randomly selected articles was undertaken to produce a draft list of topics (e.g. Therapeutics) and article types (e.g. Comment/Editorial). An iterative process was undertaken to adjust the topic and article type list to provide better coverage for the curated documents. A further 500 documents were sampled later in the pandemic and another iterative process was undertaken as new topics were appearing in larger quantities (e.g. contact tracing). Finally, manual review of the papers with high Altmetric scores identified several smaller topics that had not been captured by random sampling, including Long Haul, which were added to the list. As the coronavirus literature grows, we may need to add new topics as the research focus changes.

**Annotation:** Articles were manually annotated for topics and article types using a custom web interface. The Research article type was omitted, with the assumption that any article type without article type annotation was an Original Research article. The first 1500 randomly sampled articles were annotated during the iterative process that defined the set of topics and article types. This first set illustrated temporal skewing of topics (outlined in Fig \@ref(fig:fig1)D) as papers sampled earlier in the pandemic tended to include more Forecasting papers. A further ~1200 articles have been identified for annotation through manual identification, their high Altmetric scores or uncertainty in the machine learning system. Some of the articles were flagged using the CoronaCentral “Flag Mistake” system while others were identified through manual searching to improve representation of different topics. A final 500 articles were randomly selected and annotated for use as a held-out test set.

**BERT-based Topic & Article Type Prediction:** Cross-validation using a 75%/25% training/validation split was used to evaluate BERT-based document classifier as well as traditional methods. Topics and article types were predicted together using the title and abstract as input. Multi-label classifiers were implemented using ktrain [@maiya2020ktrain] and HuggingFace models for BERT models and scikit-learn for others [@scikitlearn]. Hyperparameter optimization involved a grid search over different models (BioBERT, BlueBERT, PubMedBert and scibert), epochs (4 to 96) and learning rate (1e-3 to 5e-6) and selecting for the highest macro F1 score. The best model used the microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract BERT model [@gu2020domain] with 32 epochs, a learning rate of 5e-05, and a batch size of 8. More details of the hyperparameter optimization are available in the GitHub repository. This model was then evaluated on the held-out test set for final performance and a full model was retrained using these parameters with all annotated documents and applied to the full coronavirus literature. To match with the annotated data where a document without a specific article type is assumed to be Original Research, any document that has not been assigned an article type by the BERT classifier is predicted as an Original Research article.

**Additional Rule-based Topic & Article Type Prediction:** Additional heuristics were used to identify the Clinical Trial topic (which is not predicted by the BERT system) and to overrule the article type prediction made by BERT when additional information was available. Clinical trials were identified through regular expression search for trial identifiers (which through validation on 100 randomly selected papers tagged as Clinical Trial, showed 93% accuracy for papers discussing trial results or protocols). For article types: book chapters were identified by obvious chapter headings in document titles, CDC Weekly Reports by the specific journal name ('MMWR. Morbidity and Mortality Weekly Report') and retractions through PubMed flags and titles beginning with "Retraction", "Retracted" or "Withdrawn". The metadata provided by the publisher’s website is combined with PubMed metadata to identify some article types, e.g. documents tagged as Commentary or Viewpoints on publisher’s websites were categorized as Comment/Editorial.

**Entity Extraction:** In order to enable users to search by a specific biomedical entity (e.g. a drug name) and for the search to capture all relevant synonyms, we extract mentions of biomedical concepts and map them back to their normalized forms with unique identifiers. This set of entity types include drug names, locations, genes/proteins, symptoms and other types. This set was refined based on entities that would be particularly relevant for different topics (e.g. Drug for Therapeutics, Symptom for Clinical Reports, etc). The lists of entities were sourced from WikiData or built manually. Entities of types Drug, Location, Symptom, Medical Specialty, and Gene/Protein are gathered from Wikidata using a series of SPARQL queries. A custom list of Prevention Methods, Risk Factors, Test Types, Transmission Types, and Vaccine Types is also constructed based on Wikidata entities. Additional customizations are made to remove incorrect synonyms. A custom list of coronavirus proteins was added to the Gene/Protein list. Exact string matching is used to identify mentions of entities in text using the Wikidata set of synonyms and a custom set of stopwords. A simple disambiguation method was used to link virus proteins with the relevant virus based on mentions of the virus elsewhere in the document. This meant that a mention of a "Spike protein" in a MERS paper would correctly link it to the MERS-CoV spike protein and not to the SARS-CoV-2 spike protein. If multiple viruses were mentioned, no disambiguation was made. A regular expression based system is used to identify mentions of Genomic Variation (e.g. D614G) and viral lineages (e.g. B.1.1.7)

**Interface:** The data is presented through a website built using NextJS with a MySQL database backend. Visualizations are generated using ChartJS and mapping using Leaflet.

**PubTator Concept Analysis:** To find the concepts that have had the largest difference in frequency, PubTator Central [@wei2019pubtator] was used as it covers a broad range of biomedical entity types such as disease, drug, and gene. It was aligned with PubMed to link publication dates to entity annotations. This used the BioText project (https://github.com/jakelever/biotext). Concept counts were calculated per publication year and normalized to percentages by total publications by year. The differences between these ordered to identify the biomedical concepts with largest change in percentage. Entity mentions of the type "Species" were removed due to lack of value as "human" dominated the data.

**Other Analyses:** All other analyses were implemented in Python and visualized using R and ggplot2.

**Code Availability:** The code for the machine learning system and paper analysis are available at https://github.com/jakelever/corona-ml. The code for the web interface is available at https://github.com/jakelever/corona-web.

**Data Availability:** The data is hosted on Zenodo and available at https://doi.org/10.5281/zenodo.4383289.

## Acknowledgements {-}

This project has been supported by the Chan Zuckerberg Biohub and through the National Library of Medicine LM05652 grant (RBA). Some of the computing for this project was performed on the Sherlock cluster. We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results.

## References {-}
